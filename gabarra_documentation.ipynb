{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./src/gabarra/img/gabarra_library_logo.png\" alt=\"Logo\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "\n",
    "### **LIBRARY DEVELOPED BY THE 2024 DATA SCIENCE CLASS** \n",
    "The bridge, Bilbao, Spain\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONTENT**\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "\n",
    "* [Introduction](#topic-1)\n",
    "* [Installation and basic usage](#topic-2)\n",
    "* [Library modules and their built-in functions](#seccion-3)\n",
    "* [Functions desciptions](#seccion-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction** <a id=\"topic-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabarara is a Python library develop by students undergoing a Data Science curriculum. It serves as a comprehensive suite of functions designed to streamline the workflow of data scientists, offering robust modules for Data Analysis, Data Processing, Data Visualization, and Machine Learning endeavors.\n",
    "\n",
    "The development lifecycle of Gabarara beggins with a stringent selection process, where the most efficient functions are curated based on their efficacy and applicability. These chosen functions undergo rigorous testing to ensure seamless operation and reliability. Subsequently, detailed documentation is prepared to provide users with a comprehensive understanding of the library's functionality and operation, ensuring clarity and ease of use for all users.\n",
    "\n",
    "The official documentation of Gabarara is structured into several sections to facilitate user comprehension and efficient utilization:\n",
    "\n",
    "Installation and basic usage: This section delineates the steps required to install the Gabarara library and provides guidance on how to set it up for use.\n",
    "Module Descriptions: Lists the functions of each module for easy recognition \n",
    "Function Descriptions: A comprehensive description of each function developed within Gabarara is provided, offering detailed insights into its functionality, parameters, and usage guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Installation and basic usage** <a id=\"topic-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation \n",
    "To install gabarra library, utilize `pip install`, the Python package manager. Open a command prompt or terminal and execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gabarra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download and install the latest version of gabarra library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage \n",
    "The library offers flexibility in usage, allowing users to either import the entire library or specify individual function modules as needed. The following scripts illustrate these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gabarra as gb\n",
    "# from gabarra import *\n",
    "# import gabarra.data_analysis as gda\n",
    "# import gabarra.data_processing as gdp\n",
    "# import gabarra.data_visualization as gdv\n",
    "# import gabarra.machine_learning as gml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LIBRARY MODULES AND ITS BUILT-IN FUNCTIONS**\n",
    "The gabarra library is composed by 4 diferents modules: Data Analysis, Data Processing, Data Visualization and Machine Learning. \n",
    "\n",
    "Each module includes the following functions:\n",
    "\n",
    "\n",
    "| Data Analysis              | Data Processing            |\n",
    "|----------------------------|----------------------------|\n",
    "| `filter_rows()`            | `create_dummies()`         |\n",
    "| `remove_outliers()`        | `fill_zeros_with_mean()`   |\n",
    "| `basic_data_analysis()`    | `fill_nans_with_mean()`    |\n",
    "| `outlier_meanSd()`         | `convert_to_numeric()`     |\n",
    "| `data_report()`            |                            |\n",
    "| `missing_values_summary()` |                            |\n",
    "\n",
    "<div style=\"page-break-after: always;\"></div>\n",
    "<div style=\"page-break-after: always;\"></div>\n",
    "\n",
    "| Data Visualization              | Machine Learning                |\n",
    "|---------------------------------|---------------------------------|\n",
    "| `missing_values_summary()`      | `linear_regression()`           |\n",
    "| `plot_numeric_distributions()`  | `calculate_metrics()`           |\n",
    "| `plot_pie_charts()`             | `unSupervisedCluster()`         |\n",
    "| `plot_interactive_line_chart()` | `gradient_boosting_regression()`|\n",
    "| `plot_interactive_pie_chart()`  | `xgboost_regression()`          |\n",
    "|                                 | `most_common_words()`           |\n",
    "|                                 | `y_generator()`                 |\n",
    "|                                 | `random_forest_regression()`    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUNCTION DESCRIPTION**\n",
    "In this segment, a detailed exposition of each function's functionality is provided, elucidating its purpose, usage, and advantages. By delving into the inner workings of the function, users gain a comprehensive understanding of its capabilities and how it can be leveraged to address specific requirements. Furthermore, insights into the outputs generated by applying the function are furnished, enabling users to discern the tangible benefits derived from its utilization. Through this comprehensive elucidation, users can make informed decisions regarding the integration of the function into their workflows, thereby maximizing its utility and effectiveness.\n",
    "\n",
    "\n",
    "### **Data Analisys Module**\n",
    "Includes functions that help you work more quickly and efficiently on tasks carried out during data analysis.\n",
    "\n",
    "[Back to content](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *filter_rows*\n",
    "The **`filter_rows()`**  function is designed to filter rows in a Pandas DataFrame based on a specified condition. This function is useful for data analysis tasks where you need to subset data to meet specific criteria.\n",
    "\n",
    "This function leverages the query method from Pandas to execute the filtering condition. The try block ensures that any exceptions raised during the filtering process are caught and reported, helping users debug potential issues with the condition string.\n",
    "\n",
    "**Technical Notes**\n",
    " * The condition parameter must be a valid string expression that Pandas' query method can parse. This includes standard comparison operators and logical operators.\n",
    " * If the condition string is not valid, an exception will be caught, and an error message will be printed.\\\n",
    " \n",
    "By providing a flexible and easy-to-use filtering mechanism, filter_rows simplifies the process of subsetting DataFrames based on dynamic conditions, enhancing the efficiency of data manipulation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows(df, condition):\n",
    "    '''\n",
    "    Filters rows in a DataFrame based on a condition.\n",
    "\n",
    "    Parameters:\n",
    "    df(pd.DataFrame): The DataFrame to filter.\n",
    "    condition (str): The condition to filter the rows. Must be a valid Pandas expression.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A filtered DataFrame that meets the specified condition.\n",
    "    '''\n",
    "    try:\n",
    "        filtered_df = df.query(condition)\n",
    "        return filtered_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error al filtrar filas: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *remove_outliers*\n",
    "The **`remove_outliers()`** function is designed to eliminate outliers from a specified column in a Pandas DataFrame. This function is particularly useful in data preprocessing, where handling outliers is essential for accurate data analysis and modeling.\n",
    "\n",
    "This function calculates the first quartile (Q1) and third quartile (Q3) to determine the interquartile range (IQR). It then computes the lower and upper fences to identify outliers. Rows with values outside these fences are excluded from the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column_name):\n",
    "     \n",
    "    '''\n",
    "        Define the remove_outliers function that takes a DataFrame df and a column_name as arguments.\n",
    "        Calculate the first quartile (Q1), which is the value that separates the lowest 25% of the data. Use nanquantile to ignore any NaN values in the column.\n",
    "        Calculate the third quartile (Q3), which is the value that separates the highest 25% of the data.\n",
    "        Calculate the lower (lower_fence) and upper (upper_fence) limits to determine what values are considered outliers. Values below lower_fence or above upper_fence are considered outliers.\n",
    "        Return a filtered DataFrame containing only the values within the calculated limits, thereby excluding outliers\n",
    "    '''\n",
    "    Q1 = np.nanquantile(df[column_name], 0.25)\n",
    "    Q3  =np.nanquantile(df[column_name], 0.75)\n",
    "    IQ = Q3 - Q1\n",
    "    lower_fence = Q1 - 1.5 * IQ\n",
    "    upper_fence = Q3 + 1.5 * IQ\n",
    "    return df[(df[column_name] <= upper_fence) & (df[column_name] >= lower_fence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *basic_data_analysis*\n",
    "\n",
    "The **`basic_data_analysis()`**  function provides a comprehensive analysis of a Pandas DataFrame, including data exploration, cleaning, visualization, and statistical analysis. This function is ideal for initial data inspection and understanding the distribution and relationships within the dataset.\n",
    "\n",
    "This function performs the following tasks:\n",
    "1. Exploration: Displays the first few rows of the DataFrame.\n",
    "2. Cleaning: Removes rows with null values and ensures numeric columns are of the correct data type.\n",
    "3. Visualization: Plots a histogram of a specified numeric column.\n",
    "4. Statistical Analysis: Provides descriptive statistics and a correlation matrix.\n",
    "5. Linear Regression: Performs a linear regression analysis on two specified columns and prints the results.\n",
    "\n",
    "By using basic_data_analysis, users can quickly gain insights into their dataset, identify potential issues, and understand underlying patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_analysis(df):\n",
    "    \"\"\"\n",
    "    Performs a basic data analysis of a Pandas Dataframe\n",
    "    Args:df (pd.DataFrame)\n",
    "    Returns:None\n",
    "    \"\"\"\n",
    "    # Explore the first records\n",
    "    print(\"Primeras filas:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Data cleaning\n",
    "    df.dropna(inplace=True)  # Remove rows with null values\n",
    "    df = df.astype({\"columna_numerica\": float})  # Fix data types\n",
    "\n",
    "    # Visualization\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(df[\"columna_numerica\"], bins=20, kde=True)\n",
    "    plt.title(\"Histograma de la columna_numerica\")\n",
    "    plt.xlabel(\"Valor\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.show()\n",
    "\n",
    "    # Statistic analysis\n",
    "    print(\"\\nEstadísticas descriptivas:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Correlation\n",
    "    print(\"\\nMatriz de correlación:\")\n",
    "    print(df.corr())\n",
    "\n",
    "    # Linear regression (Example)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df[\"columna_x\"], df[\"columna_y\"])\n",
    "    print(f\"\\nRegresión lineal: Pendiente={slope:.2f}, Intercepto={intercept:.2f}, R^2={r_value**2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *outlier_meanSd*\n",
    "\n",
    "The **`outlier_meanSd()`** function is designed to remove outliers from a specified feature in a Pandas DataFrame based on the mean and standard deviation method. This function is particularly useful for preprocessing data to ensure that extreme values do not skew the analysis or modeling results.\n",
    "\n",
    "This function performs the following steps:\n",
    "1. Calculate Mean and Standard Deviation: Computes the mean (media) and standard deviation (desEst) of the specified feature.\n",
    "2. Define Thresholds: Calculates the lower (th1) and upper (th2) thresholds using the mean and standard deviation multiplied by the specified parameter (param).\n",
    "3. Filter Data: Retains rows where the feature values fall within the defined thresholds or are null (NaN).\n",
    "4. Return Result: Returns a new DataFrame with the filtered values, ensuring that the index is reset.\n",
    "\n",
    "**Technical Notes**\n",
    "* The default parameter value of 3 is commonly used to define the range for outlier detection as values beyond three standard deviations from the mean are typically considered outliers.\n",
    "* The function includes null values in the resulting DataFrame, which can be useful for certain analysis scenarios where missing data should not be removed.\n",
    "\n",
    "By using outlier_meanSd, users can effectively preprocess their data by removing extreme values, thereby enhancing the reliability and accuracy of subsequent data analysis and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_meanSd(df, feature, param=3):  \n",
    "\n",
    "    \"\"\"\"\n",
    "    This function removes outliers and null values from a pandas DataFrame by:\n",
    "        1.Calculate the mean (media) and standard deviation (desEst) of the specified feature (feature) in the DataFrame df.\n",
    "        2.Define two thresholds (th1 and th2) using the mean and standard deviation multiplied by a parameter (param). \n",
    "        These thresholds are used to identify outliers.\n",
    "        3.Filter the original DataFrame (df) based on the following conditions:\n",
    "            a.Values must fall within the range [th1, th2].\n",
    "            b.Include any null (NaN) values.\n",
    "        4.Finally, return a new DataFrame with the filtered values.\n",
    "    \"\"\"\n",
    "    media = df[feature].mean()\n",
    "    desEst = df[feature].std()\n",
    "\n",
    "    th1 = media - desEst*param\n",
    "    th2 = media + desEst*param\n",
    "\n",
    "    return df[((df[feature] >= th1) & (df[feature] <= th2))  | (df[feature].isnull())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *data_report*\n",
    "\n",
    "The **`data_report()`** function is designed to generate a comprehensive report for a given DataFrame using the pandas library in Python. This function aims to provide a detailed overview of statistics and features pertaining to the input DataFrame, aiding in efficient data exploration and analysis.\n",
    "\n",
    "This will print out the comprehensive report for the DataFrame df, providing insights into its column names, data types, percentage of missing values, number of unique values, and cardinality percentages for each column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_report(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    df (pandas DataFrame): The input DataFrame for which the report is to be generated.\n",
    "\n",
    "    This function generates a comprehensive report for a DataFrame using the pandas library.  \n",
    "    It provides a detailed overview of statistics and features for the input DataFrame. This is \n",
    "    useful for efficient data exploration and analysis. \n",
    "        1.Column Names (`COL_N`): Creates a DataFrame called `cols` with the column names from \n",
    "        the input DataFrame `df`.\n",
    "        2.Data Types (`DATA_TYPE`): Creates another DataFrame called `types` with the data types \n",
    "        of the columns in `df`.\n",
    "        3.Missing Values (`MISSINGS (%)`): Calculates the percentage of missing values (NaN) in\n",
    "        each column and creates a DataFrame called `percent_missing_df`.\n",
    "        4.Unique Values (`UNIQUE_VALUES`): Calculates the number of unique values in each column \n",
    "        and creates a DataFrame called `unicos`.\n",
    "        5.Cardinality (`CARDIN (%)`): Computes the percentage of cardinality (number of unique \n",
    "        values relative to the DataFrame size) and creates a DataFrame called `percent_cardin_df`.\n",
    "        6.Concatenation and Transposition: Combines all the above DataFrames into one called \n",
    "        `concatenado`. Then, it transposes this DataFrame so that columns become indices and vice versa.\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    # Get the NAMES\n",
    "    cols = pd.DataFrame(df.columns.values, columns=[\"COL_N\"])\n",
    "\n",
    "    # Get the TYPES\n",
    "    types = pd.DataFrame(df.dtypes.values, columns=[\"DATA_TYPE\"])\n",
    "\n",
    "    # Get the MISSINGS\n",
    "    percent_missing = round(df.isnull().sum() * 100 / len(df), 2)\n",
    "    percent_missing_df = pd.DataFrame(percent_missing.values, columns=[\"MISSINGS (%)\"])\n",
    "\n",
    "    # Get the UNIQUE VALUES\n",
    "    unicos = pd.DataFrame(df.nunique().values, columns=[\"UNIQUE_VALUES\"])\n",
    "\n",
    "    percent_cardin = round(unicos['UNIQUE_VALUES']*100/len(df), 2)\n",
    "    percent_cardin_df = pd.DataFrame(percent_cardin.values, columns=[\"CARDIN (%)\"])\n",
    "\n",
    "    concatenado = pd.concat([cols, types, percent_missing_df, unicos, percent_cardin_df], axis=1, sort=False)\n",
    "    concatenado.set_index('COL_N', drop=True, inplace=True)\n",
    "\n",
    "    return concatenado.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *missing_values_summary*\n",
    "\n",
    "The **`missing_values_summary()`**  function is designed to generate a summary of missing values within a given DataFrame. This function aids in identifying and quantifying the extent of missing data in the DataFrame.\n",
    "\n",
    "This will print out a summary of missing values in the DataFrame df, displaying the count and percentage of missing values for each column. This information facilitates the assessment of data quality and informs decisions regarding data handling strategies, such as imputation or removal of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_summary(df):\n",
    "    \"\"\"\n",
    "    Generates a summary of missing values in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with the count and percentage of missing values per column.\n",
    "    \"\"\"\n",
    "    missing_summary = df.isnull().sum().to_frame(name='Missing Values')\n",
    "    missing_summary['Percentage'] = (missing_summary['Missing Values'] / len(df)) * 100\n",
    "    return missing_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Processing**\n",
    "Includes functions that help you work more quickly and efficiently on tasks carried out during data processing.\n",
    "\n",
    "[Back to content](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *create_dummies*\n",
    "\n",
    "The **`create_dummies()`**  function is designed to generate dummy variables for all object columns within a given DataFrame. This process is essential for preparing categorical data for machine learning models, as many algorithms require numerical inputs. The resulting DataFrame includes the original numeric variables along with the newly created dummy variables.\n",
    "\n",
    "This will print out the DataFrame transformed, which includes dummy variables for categorical columns and retains the original numeric columns. This transformation is useful for preparing the DataFrame for machine learning tasks where categorical data needs to be represented numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(df):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame and creates dummy variables for all object columns.\n",
    "    The resulting DataFrame includes the dummy variables along with the original numeric variables.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with dummy variables and numeric columns\n",
    "    \"\"\"\n",
    "    \n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    numeric_df = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "    \n",
    "    dummies_df = pd.get_dummies(df[object_cols], drop_first=False) \n",
    "\n",
    "    \n",
    "    final_df = pd.concat([numeric_df, dummies_df], axis=1)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *fill_zeros_with_mean*\n",
    "\n",
    "The **`fill_zeros_with_mean()`**  function is designed to handle zero values within a specified column of a DataFrame by replacing them with the column's mean, excluding zeros from the mean calculation. This imputation strategy helps in addressing missing or inconsistent data in numeric columns, enhancing the overall quality and reliability of the dataset.\n",
    "\n",
    "This will print out the modified DataFrame, where zero values in the specified column ('column_name') have been replaced with the mean value, enhancing data consistency and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_zeros_with_mean(df, column):\n",
    "    \"\"\"\n",
    "    Fills zero values in a specified column of a DataFrame with the column's mean, excluding zeros.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the column to be imputed.\n",
    "        column (str): The name of the column containing zero values.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with zeros replaced by the mean.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified column does not exist in the DataFrame.\n",
    "\n",
    "    Warns:\n",
    "        UserWarning: If there are no non-zero values in the column, a warning is issued\n",
    "        indicating that the mean cannot be calculated and the column remains unchanged.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_value = df[df[column] != 0][column].mean()\n",
    "    df[column] = df[column].replace(0, mean_value)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *fill_nans_with_mean*\n",
    "\n",
    "The **`fill_nans_with_mean()`**  function is designed to handle NaN (Not a Number) values within a specified column of a DataFrame by replacing them with the column's mean, excluding NaNs from the mean calculation. This imputation strategy assists in addressing missing or inconsistent data in numeric columns, enhancing the overall quality and reliability of the dataset.\n",
    "\n",
    "This will print out the modified DataFrame, where NaN values in the specified column ('column_name') have been replaced with the mean value, enhancing data consistency and integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nans_with_mean(df, column):\n",
    "    \"\"\"\n",
    "    Fills NaN (Not a Number) values in a specified column of a DataFrame with the column's mean, excluding NaNs.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the column to be imputed.\n",
    "        column (str): The name of the column containing NaN values.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with NaN values replaced by the mean.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified column does not exist in the DataFrame.\n",
    "\n",
    "    \"\"\"\n",
    "    mean_value = df[column].mean(skipna=True)\n",
    "\n",
    "    df[column] = df[column].fillna(mean_value)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *fill_zeros_with_mean*\n",
    "\n",
    "The **`convert_to_numeric()`** function facilitates the conversion of categorical variables in a pandas DataFrame into numerical features. This transformation is crucial for machine learning algorithms, as they typically require numeric inputs. The function offers various encoding options, including Label Encoding, One-Hot Encoding, and Frequency Encoding, catering to different data characteristics and modeling requirements.\n",
    "\n",
    "motive (str, optional): The type of encoding to apply. Defaults to 'LabelEncoding'.\n",
    "* LabelEncoding: Assigns a unique integer to each category.\n",
    "* OneHotEncoding: Creates binary features for each category.\n",
    "* FrequencyEncoding: Assigns a value based on category frequency.\n",
    "\n",
    "\n",
    "Functionality Overview:\n",
    "1. Validation of Encoding Motive:\n",
    "    * The function checks if the provided encoding motive is valid. If not, it raises a ValueError.\n",
    "2. Selection of Columns:\n",
    "    * If no specific columns are provided, the function selects all categorical columns using select_dtypes.\n",
    "3. Encoding Process:\n",
    "    * For each selected column, the function applies the specified encoding method (LabelEncoding, OneHotEncoding, or FrequencyEncoding).\n",
    "4. Return DataFrame:\n",
    "\n",
    "The modified DataFrame with converted categorical columns is returned.\n",
    "Exceptions:\n",
    "* ValueError: If an invalid encoding motive is provided, a ValueError is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(df, motive='LabelEncoding', columns=list):\n",
    "    \"\"\"\n",
    "    Converts categorical variables in a pandas DataFrame to numerical features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the categorical variables.\n",
    "        motive (str, optional): The type of encoding to apply. Defaults to 'LabelEncoding'.\n",
    "            - 'LabelEncoding': Assigns a unique integer to each category.\n",
    "            - 'OneHotEncoding': Creates binary features for each category.\n",
    "            - 'FrequencyEncoding': Assigns a value based on category frequency.\n",
    "        columns (list): The columns to be converted. Defaults to all categorical columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with converted categorical columns.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid 'motive' is provided.\n",
    "    \"\"\"\n",
    "\n",
    "    if motive not in ['LabelEncoding', 'OneHotEncoding', 'FrequencyEncoding']:\n",
    "        raise ValueError(f\"Invalid motive: '{motive}'. Valid options are 'LabelEncoding', 'OneHotEncoding', and 'FrequencyEncoding'.\")\n",
    "\n",
    "    if not columns:\n",
    "        columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "    for col in columns:\n",
    "        if motive == 'LabelEncoding':\n",
    "            encoder = LabelEncoder()\n",
    "            df[col] = encoder.fit_transform(df[col])\n",
    "        elif motive == 'OneHotEncoding':\n",
    "            encoder = OneHotEncoder(sparse=False)  \n",
    "            encoded_df = pd.DataFrame(encoder.fit_transform(df[[col]]), columns=[f'{col}_{c}' for c in encoder.categories_[0]])\n",
    "            df = pd.concat([df, encoded_df], axis=1).drop(col, axis=1)\n",
    "        elif motive == 'FrequencyEncoding':\n",
    "            category_counts = df[col].value_counts().to_dict()\n",
    "            df[col] = df[col].replace(category_counts)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Visualization**\n",
    "Includes functions that help you work more quickly and efficiently on tasks carried out during data visualization.\n",
    "\n",
    "[Back to content](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *missing_values_summary*\n",
    "\n",
    "The **`missing_values_summary()`** function is designed to generate a summary of missing values within a given DataFrame. This function is crucial for identifying and quantifying the extent of missing data, which is an essential step in data cleaning and preprocessing.\n",
    "\n",
    "Functionality Overview:\n",
    "* Counting Missing Values:\\\n",
    "The function computes the count of missing values (NaNs) for each column in the input DataFrame using the isnull().sum() method.\n",
    "* Calculating Percentage of Missing Values:\\\n",
    "It calculates the percentage of missing values relative to the total number of entries in each column.\n",
    "* Creating Summary DataFrame:\n",
    "    The results are organized into a DataFrame with two columns:\n",
    "    * Missing Values: The count of missing values for each column.\n",
    "    * Percentage: The percentage of missing values for each column.\n",
    "\n",
    "This will print out a summary of missing values in the DataFrame df, displaying the count and percentage of missing values for each column. This information is vital for assessing data quality and determining appropriate data handling strategies, such as imputation or removal of missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_summary(df):\n",
    "    \"\"\"\n",
    "    Generates a summary of missing values in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with the count and percentage of missing values per column.\n",
    "    \"\"\"\n",
    "    missing_summary = df.isnull().sum().to_frame(name='Missing Values')\n",
    "    missing_summary['Percentage'] = (missing_summary['Missing Values'] / len(df)) * 100\n",
    "    return missing_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *plot_numeric_distributions*\n",
    "\n",
    "The **`plot_numeric_distributions()`** function is designed to visualize the distributions of all numeric columns in a given DataFrame. It generates histograms and boxplots for each numeric column, providing insights into the data's distribution and potential outliers. Additionally, if a categorical column is specified as hue, the plots will be differentiated based on the categories in this column, enabling comparison across different groups.\n",
    "\n",
    "For each numeric column, it generates a pair of plots: a histogram and a boxplot.\n",
    "\n",
    "A histogram is plotted using sns.histplot, optionally differentiated by the hue column if specified. The histogram displays the distribution of values, with density and a kernel density estimate (KDE) overlay.\n",
    "\n",
    "A boxplot is generated using sns.boxplot, optionally differentiated by the hue column if specified. The boxplot visualizes the distribution, central tendency, and outliers of the column's values.\n",
    "\n",
    "The function uses plt.show() to display the generated plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_distributions(df, hue=None):\n",
    "    \"\"\"\n",
    "    Plots histograms and boxplots for all numeric columns in the DataFrame.\n",
    "    If a categorical column is specified as hue, the plots will be differentiated by this column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    hue (str, optional): The name of the categorical column used for differentiation. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Filter numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    # Iterate over each numeric column\n",
    "    for col in numeric_columns:\n",
    "        if col != hue:\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "            # Histogram\n",
    "            sns.histplot(data=df, x=col, hue=hue, kde=True, element='step', stat='density', ax=axes[0])\n",
    "            axes[0].set_title(f'Histogram of {col}' + (f' differentiated by {hue}' if hue else ''))\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(data=df, x=hue, y=col, ax=axes[1])\n",
    "            axes[1].set_title(f'Boxplot of {col}' + (f' differentiated by {hue}' if hue else ''))\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *plot_pie_charts*\n",
    "\n",
    "The **`plot_pie_charts()`** function is designed to create pie charts for specified columns in a pandas DataFrame. This visualization is useful for showing the proportion of categories within each selected column, providing a clear visual representation of categorical data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_charts(df, columns):\n",
    "    \"\"\"\n",
    "    Creates pie charts for the specified columns in the DataFrame.\n",
    "\n",
    "    :param df: DataFrame containing the data.\n",
    "    :param columns: List of columns for which pie charts will be created.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        # Verificar si la columna existe en el DataFrame\n",
    "        if column not in df.columns:\n",
    "            print(f\"Columna {column} no encontrada en el DataFrame.\")\n",
    "            continue\n",
    "\n",
    "        # Número de categorías en la columna\n",
    "        n_bins = df[column].nunique()\n",
    "\n",
    "        # Obtener colores viridis\n",
    "        colors = get_viridis_colors(n_bins)\n",
    "\n",
    "        # Agrupar por la columna y sumar las órdenes\n",
    "        data = df.groupby([column]).num_orders.sum()\n",
    "\n",
    "        # Crear el gráfico de pastel\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.pie(data,\n",
    "                labels=data.index,\n",
    "                shadow=False,\n",
    "                colors=colors,\n",
    "                explode=[0.05] * n_bins,\n",
    "                startangle=90,\n",
    "                autopct='%1.1f%%', pctdistance=0.9,\n",
    "                textprops={'fontsize': 8})\n",
    "        plt.title(f\"% de pedidos por {column}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *plot_interactive_line_chart*\n",
    "\n",
    "The **`plot_interactive_line_chart()`** function is designed to create an interactive line chart for specified columns in a pandas DataFrame using Plotly, a powerful graphing library. This visualization allows for interactive exploration of data trends over time or other continuous variables, with optional differentiation by a categorical column.\n",
    "\n",
    "The function checks if the specified x_column, y_column, and color_column (if provided) exist in the DataFrame. If any column is not found, an appropriate message is printed, and the function exits.\n",
    "\n",
    "The code will generate and display an interactive line chart based on the specified columns, providing an interactive way to explore data trends. If a color_column is provided, the lines will be color-coded based on the categories in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interactive_line_chart(df, x_column, y_column, color_column=None):\n",
    "    \"\"\"\n",
    "    Creates an interactive line chart for the specified columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    x_column (str): The name of the column to be used for the x-axis.\n",
    "    y_column (str): The name of the column to be used for the y-axis.\n",
    "    color_column (str, optional): The name of the column used for color differentiation. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the columns exist in the DataFrame\n",
    "    if x_column not in df.columns:\n",
    "        print(f\"Column {x_column} not found in the DataFrame.\")\n",
    "        return\n",
    "    if y_column not in df.columns:\n",
    "        print(f\"Column {y_column} not found in the DataFrame.\")\n",
    "        return\n",
    "    if color_column and color_column not in df.columns:\n",
    "        print(f\"Column {color_column} not found in the DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Create the line chart\n",
    "    fig = px.line(df, x=x_column, y=y_column, color=color_column, title=f'{y_column} over {x_column}')\n",
    "\n",
    "    # Update the layout for better appearance\n",
    "    fig.update_layout(\n",
    "        xaxis_title=x_column,\n",
    "        yaxis_title=y_column,\n",
    "        legend_title=color_column if color_column else 'Legend',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *plot_interactive_pie_chart*\n",
    "\n",
    "The **`plot_interactive_pie_chart()`** function generates an interactive pie chart for a specified column in a pandas DataFrame using Plotly. This visualization is useful for displaying the distribution of categories within a specific column, providing a clear and interactive way to understand the proportion of each category.\n",
    "\n",
    "The function checks if the specified column exists in the DataFrame. If the column is not found, an appropriate message is printed, and the function exits.\n",
    "\n",
    "The function does not return any value. It directly displays the interactive pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interactive_pie_chart(df, column):\n",
    "    \"\"\"\n",
    "    Creates an interactive pie chart for the specified column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    column (str): The name of the column for which the pie chart will be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column not in df.columns:\n",
    "        print(f\"Column {column} not found in the DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Aggregate the data\n",
    "    data = df[column].value_counts().reset_index()\n",
    "    data.columns = [column, 'counts']\n",
    "\n",
    "    # Create the pie chart\n",
    "    fig = px.pie(data, values='counts', names=column, title=f'Percentage of Orders by {column}')\n",
    "\n",
    "    # Update the layout for better appearance\n",
    "    fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Machine Learning Module**\n",
    "Includes functions that help you work more quickly and efficiently on tasks carried out during data analysis.\n",
    "\n",
    "[Back to content](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *linear_regression*\n",
    "\n",
    "The **`linear_regression()`** function performs linear regression on a given pandas DataFrame. It uses the target column specified by the user and calculates performance metrics such as Mean Squared Error (MSE) and R-squared (R²) score. This function is useful for predictive modeling and assessing the relationship between features and the target variable.\n",
    "\n",
    "The code will perform linear regression on the provided DataFrame, using the specified target column, and return a dictionary with the model, predictions, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(df:pd, target_column:str):\n",
    "    \"\"\"\n",
    "    Performs linear regression on the given dataset without standardizing the data.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    target_column (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the model, predictions, mean squared error, and R^2 score.\n",
    "    \"\"\"\n",
    "    # Splitting the data into features (X) and target (y)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Creating and training the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculating performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Returning the results\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"predictions\": y_pred,\n",
    "        \"mean_squared_error\": mse,\n",
    "        \"r2_score\": r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *calculate_metrics*\n",
    "\n",
    "The **`calculate_metrics()`** function computes and returns a DataFrame containing key regression metrics for evaluating model performance. This function is useful for summarizing the performance of regression models in a structured and easily interpretable format.\n",
    "\n",
    "The code calculates the MAE, MAPE, MSE, and RMSE for the given true and predicted values, then returns these metrics in a formatted DataFrame for easy interpretation and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test:np, predictions:np, model_name:str, decimal_places=2):\n",
    "    \"\"\"\n",
    "    Calculates and returns a DataFrame with regression metrics: MAE, MAPE, MSE, and RMSE.\n",
    "\n",
    "    Parameters:\n",
    "    y_test (array-like): True values.\n",
    "    predictions (array-like): Predicted values.\n",
    "    model_name (str): Name of the model.\n",
    "    decimal_places (int, optional): Number of decimal places to display. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    mae = metrics.mean_absolute_error(y_test, predictions)\n",
    "    mape = metrics.mean_absolute_percentage_error(y_test, predictions)\n",
    "    mse = metrics.mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Create a DataFrame with the metrics\n",
    "    data = {\"Modelo\": [model_name], 'MAE': [mae], 'MAPE': [mape], 'MSE': [mse], 'RMSE': [rmse]}\n",
    "    df_metrics = pd.DataFrame(data)\n",
    "\n",
    "    # Format the DataFrame to show specified decimal places\n",
    "    pd.options.display.float_format = f'{{:.{decimal_places}f}}'.format\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *unSupervisedCluster*\n",
    "\n",
    "The **`unSupervisedCluster()`** function leverages the KMeans algorithm for clustering analysis. It serves dual purposes based on the specified motive:\n",
    "\n",
    "1. Analysis: Visualizing the relationship between the number of clusters (k) and clustering metrics (inertia and silhouette score).\n",
    "2. Clustering: Assigning each data point to a cluster and returning the cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unSupervisedCluster(df:pd, motive='analisys', range=20 ,k=3 ):\n",
    "    \n",
    "    '''\n",
    "    Function:\n",
    "    -----------\n",
    "\n",
    "    This function works with the unsupervised model of Kmeans, and its objective is to show you how depending the number of\n",
    "    clusters that you want the inertia and the silhouette score are going to go up or down to facilitate your choose oof k, and also\n",
    "    have the model of Kmeans to see thoose clusters.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: Pandas DataFrame\n",
    "        Data that the function is going to analyze\n",
    "    motive: str\n",
    "        Depend in wich word you use the function is going to ralize different things, for example 'Analysis' show you 2 graphs\n",
    "        and 'clustering' give you in wich cluster is every target\n",
    "    Range: int\n",
    "        Range of k's that are in the graph showing the inertia and the silhouette score for each one of them\n",
    "    K: int\n",
    "        number that indicates how much clusters do you want in the modeling of Kmeans\n",
    "    Returns:\n",
    "    -----------\n",
    "    Pandas DataFrame\n",
    "        The function returns a dataframe with an aditional column wich have in wich cluster each target is in\n",
    "\n",
    "    '''\n",
    "    if motive=='analisys':\n",
    "        km_list = [KMeans(n_clusters=a, random_state=42).fit(df) for a in range(2,range)]\n",
    "        inertias = [model.inertia_ for model in km_list]\n",
    "        silhouette_score_list = [silhouette_score(df, model.labels_) for model in km_list]\n",
    "\n",
    "        plt.figure(figsize=(20,5))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        sns.set(rc={'figure.figsize':(10,10)})\n",
    "        plt.plot(range(2,range), inertias)\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(\"inertias\")\n",
    "        sns.despine()\n",
    "\n",
    "        plt.subplot(122)\n",
    "        sns.set(rc={'figure.figsize':(10,10)})\n",
    "        plt.plot(range(2,range), silhouette_score_list)\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(\"silhouette_score\")\n",
    "        sns.despine()\n",
    "\n",
    "    if motive =='clustering':\n",
    "        kmeans = KMeans(n_clusters=k,n_init=10, random_state=42).fit(df)\n",
    "        df_clusters = pd.DataFrame(kmeans.labels_, columns=['Cluster'])\n",
    "        return df_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *gradient_boosting_regression*\n",
    "\n",
    "The **`gradient_boosting_regression()`** function applies Gradient Boosting regression to a given dataset. It trains a Gradient Boosting model and computes various performance metrics to evaluate the model's predictive power.\n",
    "\n",
    "The function returns a dictionary containing the model, predictions, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_regression(df:pd, target_column:str,test_size=0.2,random_state=42,n_estimators=100,learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Performs Gradient Boosting regression on the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    target_column (str): The name of the target column.\n",
    "    test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "    random_state (int, optional): Controls the shuffling applied to the data before applying the split. Default is 42.\n",
    "    n_estimators (int, optional): The number of boosting stages to be run. Default is 100.\n",
    "    learning_rate (float, optional): Learning rate shrinks the contribution of each tree by learning_rate. Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the model, predictions, MAE, MAPE, MSE, RMSE, and R^2 score.\n",
    "    \"\"\"\n",
    "    # Splitting the data into features (X) and target (y)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Creation and training the Gradient Boosting model\n",
    "    model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Performance metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Results\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"predictions\": y_pred,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2_score\": r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *xgboost_regression*\n",
    "\n",
    "The **`xgboost_regression()`** function applies XGBoost regression to a given dataset. It trains an XGBoost regression model and computes various performance metrics to evaluate the model's predictive power.\n",
    "\n",
    "The function returns a dictionary containing the model, predictions, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_regression(df:pd, target_column:str, test_size=0.2, random_state=42, n_estimators=100, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Performs XGBoost regression on the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    target_column (str): The name of the target column.\n",
    "    test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "    random_state (int, optional): Controls the shuffling applied to the data before applying the split. Default is 42.\n",
    "    n_estimators (int, optional): The number of boosting stages to be run. Default is 100.\n",
    "    learning_rate (float, optional): Learning rate shrinks the contribution of each tree by learning_rate. Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the model, predictions, MAE, MAPE, MSE, RMSE, and R^2 score.\n",
    "    \"\"\"\n",
    "    # Splitting the data into features (X) and target (y)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Creating and training the XGBoost model\n",
    "    model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Performance metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Results\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"predictions\": y_pred,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2_score\": r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *most_common_words*\n",
    "\n",
    "The **`most_common_words()`** function extracts the most common words from a list of texts in a specified language while excluding stop words. It utilizes the CountVectorizer from scikit-learn to tokenize the texts and count the occurrences of each word.\n",
    "\n",
    "* It might be beneficial to handle case sensitivity by converting all texts to lowercase before tokenization to ensure accurate word counts.\n",
    "* Providing an option to return the most common words as a dictionary or list instead of printing them directly could enhance the function's versatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(texts:list, nwords:int, language:str):\n",
    "\n",
    "    '''\n",
    "    From a list of texts returns n most common words in a given language excluding stop words.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list): List of texts\n",
    "    nwords (int): number of most common words\n",
    "    language (str): language of the text\n",
    "\n",
    "    Return: print n most common worlds with their quantity\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> most_common_words(['I hate cats', 'I love dogs', 'My dog love cats'], 3, 'english')\n",
    "    Most common words:\n",
    "    cats: 2\n",
    "    love: 2\n",
    "    dog: 1\n",
    "    '''\n",
    "    vectorizer_count = CountVectorizer(max_features=nwords, stop_words=language)\n",
    "    texts = vectorizer_count.fit_transform(texts)\n",
    "    vocabulary = vectorizer_count.vocabulary_\n",
    "    most_common_words = {word: texts[:, index].sum() for word, index in vocabulary.items()}\n",
    "    most_common_words = sorted(most_common_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Most common words:\")\n",
    "    for word, frequency in most_common_words:\n",
    "        print(f'{word}: {frequency}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *y_generator*\n",
    "\n",
    "The **`y_generator()`** function categorizes a set of images into different classes based on their names. It maps each image to one or more labels provided in the labels parameter.\n",
    "\n",
    "Returns a list of arrays representing the labels for each image. Each array is a one-hot encoded representation of the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_generator(path, labels, separator):\n",
    "\n",
    "    ''' \n",
    "    Categorize a set of images for the training of multi-class learning models.\n",
    "    Based on the image name.\n",
    "    path[str]: Path to the folder.\n",
    "    labels[list]: Possible output labels.\n",
    "    separator[str]: Separator in the image name.\n",
    "     \n",
    "    '''\n",
    "\n",
    "    y = []\n",
    "\n",
    "    for i in os.listdir(path):\n",
    "        s = i\n",
    "        if separator:\n",
    "            s = i.split(separator)\n",
    "        j = labels\n",
    "        c =\"\"\n",
    "        for w in s:\n",
    "            for r in j:\n",
    "                if r.lower() in w.lower():\n",
    "                    c = w\n",
    "                for x,y in enumerate(labels):\n",
    "                    if y == c:\n",
    "                        arr = np.zeros(len(labels))\n",
    "                        arr[x] = 1\n",
    "                        y.append(arr)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *random_forest_regression*\n",
    "\n",
    "The **`random_forest_regression()`** function conducts Random Forest regression analysis on a provided dataset to predict a target variable. \n",
    "\n",
    "The proportion of the dataset to be used for testing. Default is 0.2. The seed used by the random number generator. Default is 42.\n",
    "The number of trees in the Random Forest. Default is 100. In any of this parameters, the values can be change for any other. \n",
    "\n",
    "Returns a dictionary containing the model object, predictions, and various performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(df, target_column,\n",
    "                              test_size=0.2,\n",
    "                                random_state=42,\n",
    "                                  n_estimators=100):\n",
    "    \"\"\"\n",
    "    Performs Random Forest regression on the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    target_column (str): The name of the target column.\n",
    "    test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "    random_state (int, optional): Controls the shuffling applied to the data before applying the split. Default is 42.\n",
    "    n_estimators (int, optional): The number of trees in the forest. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the model, predictions, MAE, MAPE, MSE, RMSE, and R^2 score.\n",
    "    \"\"\"\n",
    "    # Splitting the data into features (X) and target (y)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Creating and training the Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Performance metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Results\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"predictions\": y_pred,\n",
    "        \"MAE\": mae,\n",
    "        \"MAPE\": mape,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2_score\": r2\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
